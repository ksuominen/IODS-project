# Clustering and classification

```{r}
date()
```
  
```{r}
library(MASS)
# load the data 
data("Boston")

# check structure and dimensions
str(Boston)
dim(Boston)
```  
  
## Short description of dataset   
The dataset contains housing values in suburbs of Boston. It can be downloaded from R's MASS package, and contains 506 observations of 14 variables. More information on the data and variables can be found here:  https://stat.ethz.ch/R-manual/R-devel/library/MASS/html/Boston.html  
    
```{r}
library(tidyr)
# check summary of data
summary(Boston)

# draw pairs plot of the variables
pairs(Boston)

# calculate the correlation matrix and round it
cor_matrix <- cor(Boston)
cor_matrix %>% round(2)

# visualize the correlation matrix
library(corrplot)
corrplot(cor_matrix, method="circle")
```   
  
## Graphical overview of the data and summaries of the variables  
The summary of the data shows means, medians and ranges of the different variables, for example median number of rooms per dwelling is 6.2 (range 3.6-8.8).   
  
Tax rate and accessibility to radial highways seem to be strongly positively correlated. Median value and lower status of the population are negatively correlated. The plotted correlation matrix shows also other positive and negative correlations. The darker blue a sphere is, the stronger the positive correlation between variables, and the darker red a sphere is, the stronger the negative correlation.      
      
```{r}
library(dplyr)
# center and standardize variables
boston_scaled <- scale(Boston)

# summaries of the scaled variables
summary(boston_scaled)

# change the object to data frame
boston_scaled <- as.data.frame(boston_scaled)

boston_scaled$crim <- as.numeric(boston_scaled$crim)

# create a quantile vector of crim
bins <- quantile(boston_scaled$crim)

# create a categorical variable 'crime'
crime <- cut(boston_scaled$crim, breaks = bins, label = c("low", "med_low", "med_high", "high"), include.lowest = TRUE)

# remove original crim from the dataset
boston_scaled <- dplyr::select(boston_scaled, -crim)

# add the new categorical value to scaled data
boston_scaled <- data.frame(boston_scaled, crime)

# number of rows in the Boston dataset 
n <- nrow(boston_scaled)

# choose randomly 80% of the rows
ind <- sample(n,  size = n * 0.8)

# create train set
train <- boston_scaled[ind,]

# create test set 
test <- boston_scaled[-ind,]
```   
  
## Standardizing the dataset  
The variables are now transformed on the same scale, so now we can compare them.  
    
```{r}
# linear discriminant analysis
lda.fit <- lda(crime ~., data = train)

# print the lda.fit object
lda.fit

# the function for lda biplot arrows
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "red", tex = 0.75, choices = c(1,2)){
  heads <- coef(x)
  arrows(x0 = 0, y0 = 0, 
         x1 = myscale * heads[,choices[1]], 
         y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
  text(myscale * heads[,choices], labels = row.names(heads), 
       cex = tex, col=color, pos=3)
}

# target classes as numeric
classes <- as.numeric(train$crime)

# plot the lda results
plot(lda.fit, dimen = 2, col = classes, pch = classes)
lda.arrows(lda.fit, myscale = 2)
```   
  

```{r}
# save the correct classes from test data
correct_classes <- test$crime

# remove the crime variable from test data
test <- dplyr::select(test, -crime)

# predict classes with test data
lda.pred <- predict(lda.fit, newdata = test)

# cross tabulate the results
table(correct = correct_classes, predicted = lda.pred$class)
```   
  
## Predictions by linear discriminant analysis  
The model has correctly predicted 17 cases with low crime rate, 13 with low to medium rate, 16 with medium to high rate and 20 with high rate. It would seem that with high crime rate the predictions are most accurate, since only 1/21 is wrongly predicted.  With medium to high there is most inaccuracy.     
    
```{r}
library(ggplot2)
set.seed(123)
data(Boston)
boston_scaled <- as.data.frame(scale(Boston))

# euclidean distance matrix
dist_eu <- dist(Boston)
summary(dist_eu)

# k-means clustering
km <- kmeans(Boston, centers = 3)

# plot part of the Boston dataset with clusters
pairs(Boston[6:10], col = km$cluster)

# determine the maximum number of clusters
k_max <- 10

# calculate the total within sum of squares
twcss <- sapply(1:k_max, function(k){kmeans(Boston, k)$tot.withinss})

# visualize the results
qplot(x = 1:k_max, y = twcss, geom = 'line')

# k-means clustering
km <- kmeans(Boston, centers = 2)

# plot the Boston dataset with clusters
pairs(Boston, col = km$cluster)

# plot part of the Boston dataset with clusters
pairs(Boston[6:10], col = km$cluster)
```   
  
## Distance measures and k-means clustering  
From plotting the total within sum of squares we see that around two the value changes quite a lot, so the appropriate number of cluster would be two. The data seems to divide nicely between two clusters according to most variables.  
  
```{r}
set.seed(123)
data(Boston)
boston_scaled <- as.data.frame(scale(Boston))

# k-means clustering
km <- kmeans(Boston, centers = 3)

# linear discriminant analysis
lda.fit <- lda(km$cluster ~., data = boston_scaled)

# the function for lda biplot arrows
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "red", tex = 0.75, choices = c(1,2)){
  heads <- coef(x)
  arrows(x0 = 0, y0 = 0, 
         x1 = myscale * heads[,choices[1]], 
         y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
  text(myscale * heads[,choices], labels = row.names(heads), 
       cex = tex, col=color, pos=3)
}

# target classes as numeric
classes <- as.numeric(km$cluster)

# plot the lda results
plot(lda.fit, dimen = 2, col = classes, pch = classes)
lda.arrows(lda.fit, myscale = 2)
```  
  
## Bonus  
Variables black and rad seem to be the most influential linear separators for the clusters.  
  
```{r}
library(plotly)
lda.fit <- lda(crime ~., data = train)
model_predictors <- dplyr::select(train, -crime)

# check the dimensions
dim(model_predictors)
dim(lda.fit$scaling)

# matrix multiplication
matrix_product <- as.matrix(model_predictors) %*% lda.fit$scaling
matrix_product <- as.data.frame(matrix_product)

classes <- as.numeric(train$crime)

plot_ly(x = matrix_product$LD1, y = matrix_product$LD2, z = matrix_product$LD3, type= 'scatter3d', mode='markers', color=classes)
```  
  
WebGL actually is supported by my browser, but for some reason this isn't working.  
  
  
  
  